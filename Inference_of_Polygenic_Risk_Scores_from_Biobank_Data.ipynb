{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shz9/thai-symposium-2025/blob/main/Inference_of_Polygenic_Risk_Scores_from_Biobank_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11229893",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-06T19:17:47.358265Z",
          "start_time": "2024-04-06T19:17:47.354284Z"
        },
        "id": "11229893"
      },
      "source": [
        "<h1></h1>\n",
        "\n",
        "<h1 align=\"center\">Inference of Polygenic Risk Scores from Biobank Data</h1>\n",
        "<h2 align=\"center\">Thai Symposium & Workshop on Genomic Medicine, Therapeutics and Health</h2>\n",
        "<h3 align=\"center\">June 13th, 2025</h3>\n",
        "\n",
        "<h3 align=\"center\">Shadi Zabad, McGill University</h3>\n",
        "<h5 align=\"center\">Modified by Yue Li</h5>\n",
        "\n",
        "Quick links:\n",
        "\n",
        "* [Shadi Zabad GitHub Page](https://github.com/shz9/viprs)\n",
        "* [Yue Li Lab website](https://www.cs.mcgill.ca/~yueli/) | [Simon Gravel Lab website](https://gravellab.github.io/)\n",
        "* [`viprs` package](https://shz9.github.io/viprs/) | [`magenpy` package](https://shz9.github.io/magenpy/)\n",
        "* Manuscripts related to `viprs`/`magenpy`:\n",
        "    * [Fast and accurate Bayesian polygenic risk modeling with variational inference](https://doi.org/10.1016/j.ajhg.2023.03.009)\n",
        "    * [Toward whole-genome inference of polygenic scores with fast and memory-efficient algorithms](https://doi.org/10.1016/j.ajhg.2025.05.002)\n",
        "    * [Sparse Polygenic Risk Score Inference with the Spike-and-Slab LASSO](https://www.medrxiv.org/content/10.1101/2025.01.28.25321292v1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56879d28",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-06T20:24:53.032693Z",
          "start_time": "2024-04-06T20:24:53.028912Z"
        },
        "id": "56879d28"
      },
      "source": [
        "### ‚öôÔ∏è Environment setup / Required packages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdd5fd28",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T13:53:32.139042Z",
          "start_time": "2024-04-12T13:53:32.056011Z"
        },
        "id": "cdd5fd28"
      },
      "source": [
        "This tutorial requires:\n",
        "    \n",
        "* `python>=3.9`\n",
        "* Jupyter notebook or Google Colab.\n",
        "\n",
        "In addition to the following `python` packages:\n",
        "\n",
        "* `magenpy>=0.1.6`\n",
        "* `viprs>=0.1.3`\n",
        "* `matplotlib`\n",
        "* `seaborn`\n",
        "* `scikit-learn`\n",
        "\n",
        "#### üõ† Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ö†Ô∏è NOTE (12/06/2025): This hack might be necessary to run this notebook\n",
        "# on Google Colab. It removes pre-installed dependencies that are\n",
        "# incompatible with magenpy/viprs. You don't need to run this locally.\n",
        "\n",
        "\n",
        "# Hide output for now:\n",
        "!pip install -q numpy==1.26.4 > /dev/null 2>&1\n",
        "!pip uninstall -q rapids-dask-dependency dask-expr -yy > /dev/null 2>&1\n",
        "import numpy as np\n",
        "if int(np.__version__[0]) > 1:\n",
        "  import os\n",
        "  os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "n-7NhqIoa57z"
      },
      "id": "n-7NhqIoa57z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02c294db",
      "metadata": {
        "id": "02c294db"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/shz9/magenpy.git > /dev/null 2>&1\n",
        "!pip install -q viprs matplotlib seaborn > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc387b45",
      "metadata": {
        "id": "fc387b45"
      },
      "source": [
        "**Note**: If you encounter issues installing `viprs` or `magenpy` in your environment, please follow the installation guide on the [documentation website](https://shz9.github.io/viprs/installation/#installation). In particular, it may be helpful to setup the environment with `conda` before proceeding with the installation.\n",
        "\n",
        "#### ‚úÖ Import main packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ca2c581",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:10:05.764105Z",
          "start_time": "2024-04-12T16:10:02.879383Z"
        },
        "id": "1ca2c581"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import magenpy as mgp\n",
        "import viprs as vp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check magenpy / VIPRS versions:\n",
        "print(\"magenpy version:\", mgp.__version__)\n",
        "print(\"viprs version:\", vp.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e0f262",
      "metadata": {
        "id": "c8e0f262"
      },
      "source": [
        "# üìù Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ab1ede8",
      "metadata": {
        "id": "9ab1ede8"
      },
      "source": [
        "In Statistical Genetics, Polygenic Risk Scores (PRS) are a **predictive tool**:\n",
        "They allow us to assess someone's **lifetime risk of disease based on their DNA sequence alone**. This, of course,\n",
        "hinges on us knowing which genetic variants are associated with a given disease, and which alleles are *risk-increasing* in the population. This is why discussions of PRS and PRS methods are often tied to the main powerhouse of modern medical genetics: **Genome-wide Association Studies (GWASs)**. GWASs are a powerful methodology for detecting statistical associations between genetic variants and disease in a given study cohort. With increasing sample sizes and improved statistical methodologies in recent GWAS analyses, interest in using their insights for developing more accurate polygenic scores has become an important priority.\n",
        "\n",
        "<center><img src=\"https://github.com/yueliyl/viprs-tut/blob/main/figures/prs_intro.png?raw=1\" alt=\"drawing\" width=\"500\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25edc617",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-06T19:59:38.325379Z",
          "start_time": "2024-04-06T19:59:38.321393Z"
        },
        "id": "25edc617"
      },
      "source": [
        "### üîπ What are Polygenic Scores useful for?\n",
        "\n",
        "Among the many promising applications of polygenic risk scores in genomic medicine, the following areas have received the most attention:\n",
        "\n",
        "<center><img src=\"https://github.com/yueliyl/viprs-tut/blob/main/figures/prs_applications.png?raw=1\" alt=\"drawing\" width=\"1000\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dbd16c4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-06T21:12:56.552640Z",
          "start_time": "2024-04-06T21:12:56.548259Z"
        },
        "id": "3dbd16c4"
      },
      "source": [
        "## üîó Table of Contents\n",
        "\n",
        "This hands-on tutorial will explain how we can go about inferring polygenic scores from biobank data.\n",
        "Specifically, we will discuss the following important concepts and considerations that are important to understand\n",
        "polygenic score and the methods used to infer them:\n",
        "\n",
        "- [(0) The Data: The data needed for Inference of Polygenic Scores.](#(0)-The-Data:-The-data-needed-for-Inference-of-Polygenic-Scores.)\n",
        "    - [(0.1) Genotype data]((0.1)-Genotype-data)\n",
        "    - [(0.2) Phenotype data](#(0.2)-Phenotype-data)\n",
        "- [(1) Modeling the relationship between Genotypes and Complex Traits](#(1)-Modeling-the-Relationship-between-Genotypes-and-Complex-Traits)\n",
        "    - [(1.1) The Linear Model](#(1.1)-The-Linear-Model)\n",
        "    - [(1.2) The effect sizes $\\beta$](#(1.2)-The-effect-sizes-$\\beta$)\n",
        "    - [(1.3) Assumptions about the distribution of effect sizes](#(1.3)-Assumptions-about-the-distribution-of-effect-sizes)\n",
        "    - [(1.4) Heritability](#(1.4)-Heritability)\n",
        "- [(2) Simulating Complex Traits using `magenpy`](#(2)-Simulating-Complex-Traits-using-magenpy)\n",
        "- [(3) Inference of Polygenic Risk Scores (PRS) from Individual-level Data](#(3)-Inference-of-Polygenic-Risk-Scores-(PRS)-from-Individual-level-Data)\n",
        "    - [(3.1) Multiple Linear Regression](#(3.1)-Multiple-Linear-Regression)\n",
        "    - [(3.2) Regularized Linear Regression](#(3.2)-Regularized-Linear-Regression)\n",
        "- [(4) Inference of Polygenic Risk Scores (PRS) from GWAS Summary Statistics](#(4)-Inference-of-Polygenic-Risk-Scores-(PRS)-from-GWAS-Summary-Statistics)\n",
        "    - [(4.1) Naive PRS](#(4.1)-Naive-PRS)\n",
        "    - [(4.2) Clumping + Thresholding (C+T)](#(4.2)-Clumping-+-Thresholding-(C+T))\n",
        "    - [(4.3) Bayesian PRS (`VIPRS`)](#(4.3)-Bayesian-PRS-(VIPRS))\n",
        "- [(5) PRS Evaluation Strategies / Metrics](#(5)-PRS-Evaluation-Strategies-/-Metrics)\n",
        "- [(6) Inference of Polygenic Risk Scores (PRS) from Publicly Available GWAS Summary Statistics](#(6)-Inference-of-Polygenic-Risk-Scores-(PRS)-from-Publicly-Available-GWAS-Summary-Statistics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68123ef6",
      "metadata": {
        "id": "68123ef6"
      },
      "source": [
        "## üîπ (0) The Data: The data needed for Inference of Polygenic Scores."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b6b1dd",
      "metadata": {
        "id": "11b6b1dd"
      },
      "source": [
        "Recall that polygenic scores are a **predictive** tool: They map from the space of DNA (i.e. genetic variants) to\n",
        "to the space of phenotypes (e.g. disease status or continuous phenotype, such as height).\n",
        "\n",
        "Therefore, in order for us to infer polygenic scores and learn this mapping, we need access to **matched genotype\n",
        "and phenotype data** from the same set of individuals.\n",
        "\n",
        "<center><img src=\"https://github.com/yueliyl/viprs-tut/blob/main/figures/genetic_data.png?raw=1\" alt=\"drawing\" width=\"600\"/></center>\n",
        "\n",
        "\n",
        "### (0.1) Genotype data\n",
        "\n",
        "Learning polygenic scores requires precisely defining the input data used to generate the prediction. Similar to\n",
        "GWAS settings, **the input for each individual is the number of non-reference alleles (i.e. mutations) at a particular position in the genome**. Which allele is considered reference here is arbitrary, though typically in many applications the more common allele in the population is taken as reference.\n",
        "\n",
        "For our purposes here, we will mainly focus on **Single Nucleotide Polymorphisms (SNPs)**, that, as the name suggests, record differences in a single DNA nucleotide between individuals in the population. Furthermore, we will consider only **bi-allelic variants**: i.e. only 2 alleles, such as A and G, are present in the population.\n",
        "\n",
        "#### Example Genotype Data from 1000G Project\n",
        "\n",
        "To make things concrete, let's look at some real genotype data from the 1000G Project. The package `magenpy`, which we installed / imported at the beginning of this tutorial, is shipped with genotype data for about ~15,000 SNPs and 378 individuals of European ancestry.\n",
        "\n",
        "We will load the genotype data from disk using the `xarray` backend and inspect its characteristics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3e997c4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:10:06.412754Z",
          "start_time": "2024-04-12T16:10:05.767263Z"
        },
        "id": "f3e997c4"
      },
      "outputs": [],
      "source": [
        "from magenpy.GenotypeMatrix import xarrayGenotypeMatrix\n",
        "\n",
        "# Load the genotype matrix by providing path to BED file:\n",
        "g_matrix = xarrayGenotypeMatrix.from_file(mgp.tgp_eur_data_path())\n",
        "# Inspect the xarray object:\n",
        "g_matrix.xr_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caecfecb",
      "metadata": {
        "id": "caecfecb"
      },
      "source": [
        "This representation is known as a `DataArray` and it summarizes the matrix itself (its shape, size, etc.)\n",
        "but also the characteristics of the the two main dimensions:\n",
        "    \n",
        "* The `sample` dimension contains information about the individuals, such as their IDs, gender, etc.\n",
        "* The `variant` dimension contains information about the SNPs, such as their IDs, genomic position, reference and alternative alleles, etc.\n",
        "\n",
        "To examine the contents of the genotype matrix itself (i.e. the mutation counts), we can load the data into\n",
        "memory and represent using `numpy` arrays:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec373bbc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:10:06.973989Z",
          "start_time": "2024-04-12T16:10:06.655158Z"
        },
        "id": "ec373bbc"
      },
      "outputs": [],
      "source": [
        "np_matrix = g_matrix.to_numpy()\n",
        "print(np_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9997a20f",
      "metadata": {
        "id": "9997a20f"
      },
      "source": [
        "### ‚ö†Ô∏è Quick Exercise (1): For each variant, count how many mutations are present in the entire sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xAsXiwk4f1RH",
      "metadata": {
        "id": "xAsXiwk4f1RH"
      },
      "outputs": [],
      "source": [
        "#@title YOUR CODE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b19b7c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:10:07.532495Z",
          "start_time": "2024-04-12T16:10:07.529660Z"
        },
        "id": "02b19b7c",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title SOLUTION\n",
        "print(np_matrix.sum(axis=0)) # minor allele count\n",
        "print(np_matrix.sum(axis=0)/np_matrix.shape[0]) # minor allele freq"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "738f9536",
      "metadata": {
        "id": "738f9536"
      },
      "source": [
        "### (0.2) Phenotype data\n",
        "\n",
        "In the context of PRS, phenotype data encomposses **measurements, diagnoses, health records, or self-reports** that are specific to each individual. Historically, the most well-studied phenotypes have been relatively easy to measure and define, primarily anthropomorphic traits such as height, weight, and Body Mass Index (BMI). These phenotypes display continuous variation in the population, which is why they are often referred to as **quantitative traits**. Another class of phenotypes that have also been well studied are **case-control or binary** phenotypes, primarily disease diagnoses, such as whether the subject has been diagonsed with Type 2 Diabetes, Heart Disease, and Alzheimer.\n",
        "\n",
        "More recently, with the emergence of large-scale genetic biobanks that are paired with **Electronic Health Records (EHR)**, everything from **medication use** to **self-reports of insomnia** have been defined as phenotypes and studied in the context of GWAS and PRS. More interestingly, computational post-processing of these raw EHR records could also result in **\"computationally-defined\" phenotypes** that are useful for some downstream applications.\n",
        "\n",
        "Unfortunately, the 1000G project does not come with phenotype data for us to inspect, but we can easily simulate\n",
        "complex traits, as we will discuss in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e7c20f8",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-06T21:39:26.899290Z",
          "start_time": "2024-04-06T21:39:26.893639Z"
        },
        "id": "6e7c20f8"
      },
      "source": [
        "## üîπ (1) Modeling the Relationship between Genotypes and Complex Traits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae2cb230",
      "metadata": {
        "id": "ae2cb230"
      },
      "source": [
        "Learning a predictor between any two modalities always proceeds by first defining a model. A model is not\n",
        "a mechanism, it doesn't define precisely how the input `X` influences the output `y` in every detail. It's a way for researchers to encode our assumptions about the dependence between input and output. In the context of statistical genetics and complex traits, the dependence between genotype and phenotype was understood, analyzed, and interrogated using the linear model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81b8b483",
      "metadata": {
        "id": "81b8b483"
      },
      "source": [
        "### (1.1) The Linear Model\n",
        "\n",
        "The linear model is the simplest possible way to parametrize the dependence between the phenotype and genotype. It assumes that each genetic variant influences the phenotype **additively** (i.e. without interaction or dependence on other variants), and the **magnitude/direction of influence** is parametrized by a quantitative parameter typically denoted by $\\beta$:\n",
        "\n",
        "$$\n",
        "\\Large y_i = \\textbf{X}_i\\beta + \\epsilon_i = \\sum_{j=1}^M X_{ij}\\beta_j + \\epsilon_i\n",
        "$$\n",
        "\n",
        "This equation tells us that the phenotype of individual $i$ is the sum of their genotype (i.e. number of mutations they carry) at each position, weighted by the **effect size** $\\beta$. Here, $M$ denotes that total number of genetic variants in our study. The variable $\\epsilon$ is known as the **residual**, capturing all non-additive and non-genetic effects on the phenotype.\n",
        "\n",
        "To make things more concrete, let's focus on a simple case where there are only 2 genetic variants $X_1$ and $X_2$. We will assume that:\n",
        "\n",
        "* Variant 1 has an effect of $\\beta_1 = 10$ on the phenotype.\n",
        "* Variant 2 has an effect of $\\beta_2 = -3$ on the phenotype.\n",
        "\n",
        "Furthermore, let's assume that individual $i$ carries:\n",
        "\n",
        "* 1 mutation on the first variant $X_1$\n",
        "* 2 mutations on the second variant $X_2$\n",
        "    \n",
        "Assuming this phenotype does not have any non-genetic / non-additive influences ($\\epsilon = 0$), we get that the phenotype for individual $i$ is:\n",
        "\n",
        "$$\n",
        "\\large y_i = X_1*\\beta_1 + X_2*\\beta_2 = 1*10 + 2*(-3) = 4\n",
        "$$\n",
        "\n",
        "\n",
        "This simple model is the basis for most statistical genetics analyses and tools, from GWAS to PRS. In the context of the linear model, the **prediction (or polygenic score) for individual $i$ is simply defined as**:\n",
        "\n",
        "$$\n",
        "\\Large PRS_i = \\hat{y}_i = \\sum_{j=1}^M X_{ij}*\\hat{\\beta}_j\n",
        "$$\n",
        "\n",
        "Note that the hat $\\hat{\\beta}_j$ is used here to indicate that the effects are **estimated** from data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YIDEeUfBRart",
      "metadata": {
        "id": "YIDEeUfBRart"
      },
      "source": [
        "### (1.2) The effect sizes $\\beta$\n",
        "\n",
        "As mentioned above, the $\\beta$ parameter is a simple weight that captures the magnitude and direction of the effect on phenotype. The magnitude (absolute value) of the effect ($|\\beta|$) tells us about the strength of this effect. In the example above, variant 1 had a much larger effect, in absolute terms, on the phenotype than variant 2. The direction (i.e. sign) also conveys important information:\n",
        "\n",
        "* Variants / alleles with **positive $\\beta$ will increase** the phenotype.\n",
        "* Variants / alleles with **negative $\\beta$ will decrease** the phenotype.\n",
        "\n",
        "**Note**: We say \"variants / alleles\" here because this depends on how we choose our \"reference allele\" when we construct the genotype matrix. If the reference allele is changed, the effect direction will flip.\n",
        "\n",
        "A natural consequence of the above definitions is that **variants whose weight is set to zero (i.e. $\\beta = 0$) will have no effect on the phenotype**.\n",
        "\n",
        "A **central question in statistical genetics concerns the genetic architecture of complex traits: The distribution of effect sizes for a particular trait or disease**. Some of the questions that are asked in this context:\n",
        "\n",
        "* How many variants have **non-zero weights** (i.e. $\\beta \\neq 0$)? This quantity is known as **polygenicity**.\n",
        "* How does the **distribution of variant effect sizes vary by genomic contexts**?\n",
        "    * E.g. Do variants in coding or regulatory regions have larger effect magnitudes than the rest of the genome?\n",
        "    * These questions are studied in the context of **stratified SNP-heritability**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NCQPkyuuRjkf",
      "metadata": {
        "id": "NCQPkyuuRjkf"
      },
      "source": [
        "### (1.3) Assumptions about the distribution of effect sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DJCiJlmDUbZ5",
      "metadata": {
        "id": "DJCiJlmDUbZ5"
      },
      "source": [
        "#### (1.3.1) The infinitesimal model\n",
        "\n",
        "In the early days of statistical genetics, the most commonly used models assumed that most or all variants had an effect on the phenotype. This is sometimes known as the **infinitesimal model**. A common way to parametrize this belief is to assume that the effect sizes for all variants are drawn from a Gaussian distribution with mean zero and variance set to the per-SNP heritability (to be discussed in the next section):\n",
        "\n",
        "$$\n",
        "\\Large \\beta_j \\sim N(0, \\frac{h^2}{M})\n",
        "$$\n",
        "\n",
        "**NOTE**: This formulation assumes total phenotypic variance $\\sigma^2_y = h^2 + \\sigma^2_\\epsilon$ is set to 1. More on this later.\n",
        "\n",
        "To illustrate what this model implies in practice, let's try and simulate it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0178aa04",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:10:11.055296Z",
          "start_time": "2024-04-12T16:10:10.638258Z"
        },
        "id": "0178aa04"
      },
      "outputs": [],
      "source": [
        "def sample_beta_infinitesimal(n_snps, h2, random_seed=7209):\n",
        "    \"\"\"\n",
        "    Sample variant effect sizes (BETA) according to the\n",
        "    infinitesimal model.\n",
        "\n",
        "    :param n_snps: The number of genetic variants.\n",
        "    :param h2: The heritability for the phenotype of interest (float from 0 to 1).\n",
        "    :param random_seed: The seed for the random number generator.\n",
        "    \"\"\"\n",
        "\n",
        "    # Sanity checks\n",
        "    assert 0. <= h2 <= 1.\n",
        "\n",
        "    per_snp_h2 = h2 / n_snps\n",
        "\n",
        "    return np.random.normal(\n",
        "        loc=0.,  # The mean\n",
        "        scale=np.sqrt(per_snp_h2),  # sqrt(variance)\n",
        "        size=n_snps  # The number of variants\n",
        "    )\n",
        "\n",
        "# Sample effect sizes according to the infinitesimal model\n",
        "# where the heritability is set to 0.5 and the number of variants\n",
        "# is set to 10k:\n",
        "beta = sample_beta_infinitesimal(n_snps=10_000, h2=0.5)\n",
        "\n",
        "# Plot a histogram of the effect sizes:\n",
        "plt.hist(beta, bins=50, label='$h^2=0.5$')\n",
        "plt.xlabel(\"BETA\")\n",
        "plt.title(\"Infinitesimal model\\nDistribution of variant effect sizes ($M=10000$)\");\n",
        "\n",
        "# Now try simulate with smaller h2 as a comparison\n",
        "plt.hist(sample_beta_infinitesimal(n_snps=10_000, h2=0.01), bins=50, label='$h^2=0.1$')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97aa13dc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-06T23:37:45.897675Z",
          "start_time": "2024-04-06T23:37:45.890790Z"
        },
        "id": "97aa13dc"
      },
      "source": [
        "#### (1.3.2) The spike-and-slab prior\n",
        "\n",
        "From an an evolutionary perspective, as well as given our current understanding of genetics from molecular and genetic epidemiology studies, we know that most variants have no or negligible effect on most phenotypes. This evidence and observations motivated an alternative model that gives parametric form to this \"**sparsity**\": The **spike-and-slab prior**.\n",
        "    \n",
        "The spike-and-slab is a mixture distribution: Instead of having a single Gaussian distribution from which\n",
        "we draw the effect size, we are going to have two distributions:\n",
        "    \n",
        "* **The spike distribution** $\\delta_0$: The spike distribution is a degenerate density where all of its\n",
        "    weight is centered at zero. Visually, this distribution looks like a \"spike\" at the value zero,\n",
        "    hence the name. As we'd expect, the spike distribution is used to model variants\n",
        "    with no or negligible effect on the phenotype.\n",
        "\n",
        "* **The slab distribution** $N(0, \\frac{h^2}{\\pi M})$: The slab distribution is a Gaussian distribution similar to\n",
        "    one we encountered before with the infinitesimal model. It's called a \"slab\" because, visually,\n",
        "    it looks flatter compared to the spike. The slab distribution is used to model variants with\n",
        "    an effect on the phenotype.\n",
        "    \n",
        "The other component that goes into defining the spike-and-slab is a **mixing weight**, denoted by $\\pi$,\n",
        "that tells us **what proportion of variants come from the slab distribution** (i.e. what proportion of variants\n",
        "have an effect on the phenotype). Mathematically, this is distribution is often written as:\n",
        "\n",
        "$$\\large\n",
        "\\begin{align}\n",
        "  s_j &\\sim \\pi^{[s_j=1]}(1-\\pi)^{[s_j=0]}\\\\\n",
        "  \\beta_j &\\sim s_j N(0, \\frac{h^2}{\\pi M}) + (1 - s_j)\\delta_0\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**NOTE**: Unlike in the infinitesimal model, the variance of the slab distribution is proportional to per-SNP heritability of causal variants (i.e. we scale $M$ by $\\pi$ or $M\\pi = \\sum^M_{j=1}\\mathbb{E}[s_j]$).\n",
        "\n",
        "#### Data generative process\n",
        "\n",
        "In terms of the data generative process, mixture models are usually thought of as hierarchical models:\n",
        "\n",
        "1) Determine which mixture component the variant effect comes from:\n",
        "  * This is done by sampling $s_j$ from a **Bernoulli with success probability** $\\pi$.\n",
        "  * If the Bernoulli draw is a success, then the variant is assigned to the slab; otherwise it's assigned to the spike.\n",
        "\n",
        "2) Draw the effect according to the mixture component that the variant was assigned in Step 1.\n",
        "\n",
        "\n",
        "To illustrate what the spike-and-slab model implies in practice, let's try and simulate it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca240aaf",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:10:12.492306Z",
          "start_time": "2024-04-12T16:10:12.204206Z"
        },
        "id": "ca240aaf"
      },
      "outputs": [],
      "source": [
        "def sample_beta_spike_and_slab(n_snps, h2, pi, random_seed=7209):\n",
        "    \"\"\"\n",
        "    Sample variant effect sizes (BETA) according to the\n",
        "    infinitesimal model.\n",
        "\n",
        "    :param n_snps: The number of genetic variants.\n",
        "    :param h2: The heritability for the phenotype of interest (float ranging from 0 to 1).\n",
        "    :param pi: The proportion of causal variants (float ranging from 0 to 1)\n",
        "    :param random_seed: The seed for the random number generator.\n",
        "    \"\"\"\n",
        "\n",
        "    # Sanity checks\n",
        "    assert 0. <= h2 <= 1.\n",
        "    assert 0. <= pi <= 1.\n",
        "\n",
        "    per_snp_h2 = h2 / (pi*n_snps)\n",
        "\n",
        "    # Draw mixture assignment from Bernoulli:\n",
        "    # NOTE: In numpy Bernoulli sampling is done via binomial with number of trials set to 1.\n",
        "    mixture_assignment = np.random.binomial(1, pi, size=n_snps)\n",
        "\n",
        "    # NOTE: For ease of implementation, we draw effect for all variants from\n",
        "    # slab, but then we zero the effect of non-slab variants\n",
        "\n",
        "    # Draw from Gaussian:\n",
        "    beta = np.random.normal(loc=0., # mean\n",
        "                            scale=np.sqrt(per_snp_h2), # sqrt(var)\n",
        "                            size=n_snps) # number of variants\n",
        "\n",
        "    # Final beta:\n",
        "    beta *=mixture_assignment\n",
        "\n",
        "    return beta\n",
        "\n",
        "\n",
        "# Sample betas from the spike-and-slab model:\n",
        "beta = sample_beta_spike_and_slab(n_snps=10_000,\n",
        "                                  h2=0.5,\n",
        "                                  pi=0.5)\n",
        "\n",
        "\n",
        "# Plot a histogram of the effect sizes:\n",
        "\n",
        "plt.hist(beta, bins=50)\n",
        "plt.xlabel(\"BETA\")\n",
        "plt.title(f\"Spike-and-Slab model\\nDistribution of variant effect sizes ($h^2={0.5}$; $M={10000}$; $\\pi={0.5}$)\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b078abab",
      "metadata": {
        "id": "b078abab"
      },
      "source": [
        "### (1.4) Heritability and R-squared\n",
        "\n",
        "The concept of heritability is defined as **the proportion of phenotypic variance attributable to genetics**:\n",
        "\n",
        "$$\n",
        "\\large h^2 = \\frac{\\sigma^2_g}{\\sigma^2_y}\n",
        "$$\n",
        "\n",
        "Hence, its value ranges between 0 and 1. When genetic factors explain very little of phenotypic variation in a given sample, heritability will tend towards 0. If, on the other hand, genetic factors explain most observed variation in the phenotype, its value will tend towards 1.\n",
        "\n",
        "Another important concept to understand in this context is **narrow-sense heritability** ($h^2_A$): which is defined as the proportion of phenotypic variance explained by **additive genetic effects**. And here is where the linear model comes back again: When we talk about narrow-sense heritability, we're mainly concerned about the variance of $X\\beta$ as a proportion of phenotypic variance $\\sigma^2_y=Var(X\\beta + \\epsilon)$:\n",
        "\n",
        "$$\n",
        "\\large h^2_A = \\frac{Var(X\\beta)}{\\sigma^2_y}\n",
        "$$\n",
        "\n",
        "In other words, with narrow-sense heritability, we're not concerned about variation arising due to dominance, epistasis, or other non-linear genetic effects.\n",
        "\n",
        "For the purposes of our discussions in this tutorial, heritability factors in in **two important ways**:\n",
        "\n",
        "1) Heritability is important to define the **residual component** ($\\epsilon$) in the linear model above. In most statistical genetics models, the residual effect is modelled with a Gaussian density with mean zero and variance\n",
        "set to the total phenotypic variance not captured by genetics:\n",
        "\n",
        "$$\n",
        "\\large \\epsilon_i \\sim N(0, \\sigma^2_y*(1 - h^2))\n",
        "$$\n",
        "\n",
        "For example, if the total phenotypic variance $\\sigma^2_y$ is 1 and the heritability of the phenotype is 0.5, then the variance of $\\epsilon$ will simply be set to 0.5.\n",
        "\n",
        "2) Narrow-sense heritability provides us with an **upper bound for how well we can predict the phenotype using linear models**. In statistical genetics, we often use the coefficient of determination (R-Squared or $R^2$) to evaluate model performance. R-squared is the proportion of phenotypic variance explained by the predictor:\n",
        "\n",
        "$$\\large\n",
        "  R^2 = 1 - \\frac{\\sum_i(y_i-\\hat{y}_i)^2}{\\sum_i(y_i - \\bar{y})^2}\n",
        "  = 1 - \\frac{SS_{res}}{SS_{tot}} \\le h^2\n",
        "$$\n",
        "\n",
        "Thus, for these predictors, the $R^2$ performance is upper-bounded by narrow-sense heritability. Intuitively, if the phenotype is strongly heritable, genetic predictors can in principle perform well. On the other hand, if the phenotype is mildly heritable, prediction accuracy will be poor."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bbe5dd1",
      "metadata": {
        "id": "1bbe5dd1"
      },
      "source": [
        "## ‚ö†Ô∏è Exercise (2): Simulating data according to the linear model\n",
        "\n",
        "Now that we learned all about linear models, heritability, mixture distributions, and spike-and-slab priors, let's take some time to practice simulating complex traits / phenotypes. In this exercise, you'll be given genotype data for a subset of 378 European individuals from the 1000G project. For each individual, we have mutation counts at ~15,000 variants on Chromosome 22.\n",
        "\n",
        "Your task is to simulate a phenotype for these individuals according to the following criteria:\n",
        "\n",
        "1) The **heritability** for the phenotype should be set to $h^2 = 0.5$\n",
        "\n",
        "2) Use the **spike-and-slab** model to simulate the effect sizes.\n",
        "    * Mixing proportion $\\pi$ should be set to $\\pi = 0.1$ (i.e. only 10% of the variants influence the trait).\n",
        "    \n",
        "3) Total phenotypic variance $\\sigma_y^2$ should be set to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e9b3aad",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:10:14.455504Z",
          "start_time": "2024-04-12T16:10:14.342633Z"
        },
        "id": "3e9b3aad"
      },
      "outputs": [],
      "source": [
        "#@title YOUR CODE\n",
        "# Let's define the parameters of the model:\n",
        "\n",
        "h2 = 0.5  # Heritability\n",
        "pi = 0.1  # Mixing proportion\n",
        "sigma2_y = 1.  # Phenotypic variance\n",
        "\n",
        "# Extract and standardize the genotype matrix:\n",
        "X = g_matrix.to_numpy().astype(np.float32)\n",
        "# Standardize the genotype matrix:\n",
        "X -= X.mean(axis=0)\n",
        "X /= X.std(axis=0)\n",
        "\n",
        "# Get the sample size and number of variants in\n",
        "# in the genotype matrix:\n",
        "\n",
        "n = X.shape[0] # Number of samples\n",
        "n_snps = X.shape[1] # Number of variants\n",
        "\n",
        "################ STEP 1 ################\n",
        "# Sample the effect sizes from spike-and-slab:\n",
        "\n",
        "# FILL IN HERE\n",
        "\n",
        "################ STEP 2 ################\n",
        "# Compute the polygenic score (X\\beta)\n",
        "\n",
        "# FILL IN HERE\n",
        "\n",
        "\n",
        "################ STEP 3 ################\n",
        "# Sample the residual component (epsilon)\n",
        "\n",
        "# FILL IN HERE\n",
        "\n",
        "################ STEP 4 ################\n",
        "# Compute the final phenotype according to the linear model:\n",
        "\n",
        "# FILL IN HERE\n",
        "\n",
        "################ STEP 5 ################\n",
        "# Plot the distribution of the phenotype:\n",
        "\n",
        "# FILL IN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SVS3SkNy2ahm",
      "metadata": {
        "cellView": "form",
        "id": "SVS3SkNy2ahm"
      },
      "outputs": [],
      "source": [
        "#@title SOLUTION (Don't look at it until you finish the exercise)\n",
        "# Let's define the parameters of the model:\n",
        "\n",
        "h2 = 0.5  # Heritability\n",
        "pi = 0.1  # Mixing proportion\n",
        "sigma2_y = 1.  # Phenotypic variance\n",
        "\n",
        "# Extract and standardize the genotype matrix:\n",
        "X = g_matrix.to_numpy().astype(np.float32)\n",
        "# Standardize the genotype matrix:\n",
        "X -= X.mean(axis=0)\n",
        "X /= X.std(axis=0)\n",
        "\n",
        "# Get the sample size and number of variants in\n",
        "# in the genotype matrix:\n",
        "\n",
        "n = X.shape[0] # Number of samples\n",
        "n_snps = X.shape[1] # Number of variants\n",
        "\n",
        "################ STEP 1 ################\n",
        "# Sample the effect sizes from spike-and-slab:\n",
        "\n",
        "beta = sample_beta_spike_and_slab(n_snps=n_snps,\n",
        "                                  h2=h2,\n",
        "                                  pi=pi)\n",
        "\n",
        "################ STEP 2 ################\n",
        "# Compute the polygenic score (X\\beta)\n",
        "\n",
        "y_g = X @ beta\n",
        "\n",
        "\n",
        "################ STEP 3 ################\n",
        "# Sample the residual component (epsilon)\n",
        "\n",
        "eps = np.random.normal(loc=0.,\n",
        "    scale=np.sqrt(sigma2_y - h2), #std of residual\n",
        "    size=n) # number of samples\n",
        "\n",
        "################ STEP 4 ################\n",
        "# Compute the final phenotype according to the linear model:\n",
        "\n",
        "y = y_g + eps\n",
        "\n",
        "################ STEP 5 ################\n",
        "# Plot the distribution of the phenotype:\n",
        "plt.hist(y, bins=50)\n",
        "plt.xlabel(\"Phenotype\")\n",
        "plt.ylabel(\"Samples\")\n",
        "plt.title(\"Simulated phenotype distribution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0474c4d",
      "metadata": {
        "id": "b0474c4d"
      },
      "source": [
        "## üîπ (2) Simulating Complex Traits using `magenpy`\n",
        "\n",
        "Now that we understand the data generative process better, we can avoid having to do this low-level work\n",
        "ourselves and instead delegate it to [magenpy](https://shz9.github.io/magenpy/). `magenpy` is a `python` package for statistical genetics applications and it supports various phenotype simulation tools.\n",
        "\n",
        "For example, to simulate a phenotype with the same parameters as the exercise above, we can simply invoke:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "299d39ba",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:10:21.064012Z",
          "start_time": "2024-04-12T16:10:15.528294Z"
        },
        "id": "299d39ba"
      },
      "outputs": [],
      "source": [
        "# Set random seed:\n",
        "np.random.seed(7209)\n",
        "\n",
        "# Initialize the simulator object:\n",
        "sim = mgp.PhenotypeSimulator(mgp.tgp_eur_data_path(),\n",
        "                             min_mac=10,\n",
        "                             h2=0.8,\n",
        "                             pi=0.1)\n",
        "# Filter to a smaller subset of variants:\n",
        "sim.filter_snps(sim.snps[22][:300])\n",
        "\n",
        "# Simulate a phenotype:\n",
        "sim.simulate()\n",
        "\n",
        "# Get a table with the simulated phenotype:\n",
        "pheno_table = sim.to_phenotype_table()\n",
        "\n",
        "# Display the phenotype table:\n",
        "display(pheno_table)\n",
        "\n",
        "# Plot a histogram of the simulated phenotype:\n",
        "\n",
        "plt.hist(pheno_table.phenotype, bins=20)\n",
        "plt.xlabel(\"Simulated Quantitative Trait\")\n",
        "plt.title(f\"Distribution of {sim.phenotype_likelihood} trait\\n Spike-and-slab model ($h^2={sim.h2}$; $M={sim.n_snps}$; $\\pi={sim.pi}$)\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b007389",
      "metadata": {
        "id": "9b007389"
      },
      "source": [
        "In addition to saving the simulated phenotype for us, we can also examine the simulated effect sizes and the\n",
        "results of the simulation from the spike-and-slab prior. To be specific, the package records which mixture\n",
        "component each variant was assigned to, its per-SNP heritability, its sampled effect size ($\\beta$), and whether it comes from the a non-spike mixture component (`Causal` column):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3cca6ad",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:10:22.561921Z",
          "start_time": "2024-04-12T16:10:22.275956Z"
        },
        "id": "d3cca6ad"
      },
      "outputs": [],
      "source": [
        "# Get the simulated BETAs:\n",
        "effect_table = sim.to_true_beta_table()\n",
        "\n",
        "# Display the table of effect sizes:\n",
        "display(effect_table)\n",
        "\n",
        "# Plot the distribution of simulated BETAs:\n",
        "plt.hist(effect_table.BETA, bins=30)\n",
        "plt.xlabel(\"Simulated BETA\")\n",
        "plt.title(f\"Distribution of variant effect sizes\\n Spike-and-slab model ($h^2={sim.h2}$; $M={sim.n_snps}$; $\\pi={sim.pi}$)\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f163d0b4",
      "metadata": {
        "id": "f163d0b4"
      },
      "source": [
        "### Train / test split\n",
        "\n",
        "One useful feature that `magenpy` provides is to allow us to split the simulated data into independent training\n",
        "and test sets, which we can use for robustly evaluating the predictive performance of PRS models. This can be done\n",
        "by simply invoking the `.split_by_samples()` method on the simulated data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eb4e95b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:10:23.348470Z",
          "start_time": "2024-04-12T16:10:23.309070Z"
        },
        "id": "8eb4e95b"
      },
      "outputs": [],
      "source": [
        "# Set random seed:\n",
        "np.random.seed(7209)\n",
        "\n",
        "# Split the simulated data into 75% training and 25% testing:\n",
        "train, test = sim.split_by_samples([0.75, 0.25])\n",
        "\n",
        "# Print the number of samples in the training and test sets:\n",
        "print(\"Number of samples in training set:\", train.n)\n",
        "print(\"Number of samples in the test set:\", test.n)\n",
        "\n",
        "# To streamline following steps, extract input / output data:\n",
        "\n",
        "# Training data:\n",
        "X_train = train.genotype[22].to_numpy()\n",
        "y_train = train.sample_table.phenotype\n",
        "\n",
        "# Test data:\n",
        "X_test = test.genotype[22].to_numpy()\n",
        "y_test = test.sample_table.phenotype"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ae06346",
      "metadata": {
        "id": "4ae06346"
      },
      "source": [
        "## üîπ (3) Inference of Polygenic Risk Scores (PRS) from Individual-level Data\n",
        "\n",
        "Now that we understand better the concepts underlying polygenic risk scores, let us turn to how we might go\n",
        "about estimating them from biobank data. **In practical terms, inferring polygenic scores means obtaining an estimate for the weights of the linear model $\\beta$**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MSXepN1DdW_z",
      "metadata": {
        "id": "MSXepN1DdW_z"
      },
      "source": [
        "### (3.1) Multiple Linear Regression\n",
        "\n",
        "The straightforward approach for performing inference in this context is to simply **fit a linear model to paired genotype and phenotype data**. The optimal $\\beta$s are those the minimize the **mean squared error** across across individuals in the training set:\n",
        "\n",
        "$$\n",
        "\\large MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - X_i\\beta)^2\n",
        "$$\n",
        "\n",
        "How is this achieved in practice? Solving the above objective function for the $\\beta$s gives a closed-form solution:\n",
        "\n",
        "$$\n",
        "\\large \\hat{\\beta} = (X^\\top X)^{-1}(X^\\top y)\n",
        "$$\n",
        "\n",
        "In `python`, this can be done in a variety of ways, but here we will show you how to do it using the popular `scikit-learn` package. `scikit-learn` provides a variety of interfaces for fitting\n",
        "linear models to data, all of them under than `sklearn.linear_model` module.\n",
        "\n",
        "Let's start by first fitting a simple linear model to our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ba85f3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:10:25.008026Z",
          "start_time": "2024-04-12T16:10:24.487063Z"
        },
        "id": "85ba85f3"
      },
      "outputs": [],
      "source": [
        "# Import the LinearRegression module:\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# ------------ Model fitting ------------\n",
        "# Initialize the LinearRegression object:\n",
        "prs_lm = LinearRegression()\n",
        "# Fit the model to the training data:\n",
        "prs_lm.fit(X_train, y_train)\n",
        "\n",
        "# ------------ Evaluation ------------\n",
        "\n",
        "def plot_scatter(true, pred, title=None):\n",
        "    \"\"\"\n",
        "    Helper function to plot the PRS vs. phenotype\n",
        "    and display the R-Squared metric.\n",
        "    \"\"\"\n",
        "\n",
        "    from scipy.stats import linregress\n",
        "    b, a, r_val, _, _ = linregress(true, pred)\n",
        "\n",
        "    x = np.linspace(true.min(), true.max(), 100)\n",
        "\n",
        "    # Add zero lines:\n",
        "    plt.axvline(0., ls='--', color='gray')\n",
        "    plt.axhline(0., ls='--', color='gray')\n",
        "    # Plot true vs. predicted:\n",
        "    plt.scatter(true, pred)\n",
        "\n",
        "    # Plot the fitted line:\n",
        "    plt.plot(x, x*b + a, label=f\"$R^2 = {round(r_val**2, 2)}$\", c='red')\n",
        "\n",
        "    # Add labels / title:\n",
        "    plt.xlabel(\"True Phenotype\")\n",
        "    plt.ylabel(\"PRS (Predicted Phenotype)\")\n",
        "    plt.legend()\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "# Predict on the test set:\n",
        "prs_lm_pred = prs_lm.predict(X_test)\n",
        "\n",
        "# Let's compare the model prediction to the simulated phenotypes:\n",
        "plot_scatter(y_test, prs_lm_pred, title=\"LinearRegression Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50f79915",
      "metadata": {
        "id": "50f79915"
      },
      "source": [
        "As we see in the example above, a major limitation of the using the vanilla linear model outlined above is that, given the high-dimensionality of genetic data and the strong correlations (Linkage-Disequilibrium - LD) between genetic variants, the solution often becomes becomes ill-defined. This usually happens in general regression contexts where the number of features is much larger than the number of samples and where the features can be highly correlation with each other (**multicollinearity**).\n",
        "\n",
        "**Intuition**: Why the solution fails? Recall the solution to the system of linear equations is:\n",
        "\n",
        "\n",
        "$$\n",
        "\\large \\hat{\\beta} = (X^\\top X)^{-1}(X^\\top y)\n",
        "$$\n",
        "\n",
        "This solution assumes that the $X^\\top X$ matrix (e.g. LD matrix) is invertible. However, when the number of features is much larger than the number of samples or there are highly correlated features, this matrix is no longer invertible, as we can verify with `numpy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "691397d6",
      "metadata": {
        "id": "691397d6"
      },
      "outputs": [],
      "source": [
        "# Attempt to invert the X'X matrix:\n",
        "from numpy.linalg import LinAlgError\n",
        "\n",
        "try:\n",
        "  np.linalg.inv(X_train.T.dot(X_train)) # LinAlgError\n",
        "except LinAlgError:\n",
        "  print(\"‚ùå X'X matrix is not invertible (Singular matrix)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7821383",
      "metadata": {
        "id": "f7821383"
      },
      "source": [
        "### (3.2) Regularized Linear Regression\n",
        "\n",
        "A standard way to resolve these issues with the linear model in practice is to use **regularization techniques**, such as **Ridge regression** or the **Lasso**. Regularization provides a way to incorporate preference for sparse or small effect sizes when solving for the optimal $\\beta$S. This is typically done by adding a \"penalty\" on the values of the effect sizes to the optimization objective:  \n",
        "\n",
        "$$\n",
        "\\large \\text{Ridge objective:} \\frac{1}{N} \\sum_{i=1}^N (y_i - X_i\\beta)^2 + \\alpha \\underbrace{\\sum_{j=1}^M \\beta^2_j}_{Penalty}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\large \\text{Lasso objective:} \\frac{1}{N} \\sum_{i=1}^N (y_i - X_i\\beta)^2 + \\alpha \\underbrace{\\sum_{j=1}^M |\\beta_j|}_{Penalty}\n",
        "$$\n",
        "\n",
        "Here, the parameter $\\alpha$ determines the strength of the penalty. Larger $\\alpha$ will tend to impose stronger shrinkage on the effect sizes. In `scikit-learn`, fitting regularized models can be done in very similar ways to what we have seen before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1425b97",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:10:26.973476Z",
          "start_time": "2024-04-12T16:10:26.374533Z"
        },
        "id": "b1425b97"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "# ------------ Model fitting ------------\n",
        "# Initialize the Ridge object:\n",
        "prs_ridge = Ridge(alpha=1e-3)\n",
        "# Fit the model to the training data:\n",
        "prs_ridge.fit(X_train, y_train)\n",
        "\n",
        "# Initialize the Lasso object:\n",
        "prs_lasso = Lasso(alpha=1e-3)\n",
        "# Fit the model to the training data:\n",
        "prs_lasso.fit(X_train, y_train)\n",
        "\n",
        "# ------------ Evaluation ------------\n",
        "\n",
        "# Predict on the test set:\n",
        "prs_ridge_pred = prs_ridge.predict(X_test)\n",
        "prs_lasso_pred = prs_lasso.predict(X_test)\n",
        "\n",
        "# Let's compare the model prediction to the simulated phenotypes:\n",
        "plot_scatter(y_test, prs_ridge_pred, title=\"Ridge Model\")\n",
        "plt.show()\n",
        "plot_scatter(y_test, prs_lasso_pred, title=\"Lasso Model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11c0df0e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T03:02:04.111258Z",
          "start_time": "2024-04-12T03:02:04.000811Z"
        },
        "id": "11c0df0e"
      },
      "source": [
        "### Existing PRS Methods that operate on individual-level data\n",
        "\n",
        "* [bayesR](https://github.com/syntheke/bayesR)\n",
        "* [snpnet](https://github.com/junyangq/snpnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "624ddf29",
      "metadata": {
        "id": "624ddf29"
      },
      "source": [
        "## üîπ (4) Inference of Polygenic Risk Scores (PRS) from GWAS Summary Statistics\n",
        "\n",
        "While individual-level data is valuable and offers the best and most straightforward way to infer accurate polygenic scores, in many cases, access to this data is restricted due to privacy or other concerns. Even when large-scale individual-level data is available, as in the case of the UK Biobank, we'd usually be restricted to using one biobank or one data source at a time for performing inference, which can severely limit the sample sizes that can be obtained. These concerns, among others, motivated researchers to develop ways to infer polygenic scores from GWAS summary statistics alone. The benefits of this are:\n",
        "\n",
        "1) **Protect individual privacy**: No need to access individual-level data. These analyses can be done via publicly available data only.\n",
        "2) **Larger sample sizes**: Can Leverage much larger sample sizes via meta-analysis of many smaller GWAS studies.\n",
        "3) **Computational efficiency**: It can be much faster and computationally efficient to infer polygenic scores in this way.\n",
        "\n",
        "\n",
        "### Intuition\n",
        "\n",
        "To see how polygenic scores can be inferred from GWAS summary statistics, let us turn back to the solution of the linear model we discussed previously:\n",
        "\n",
        "$$\n",
        "\\large \\hat{\\beta} = (X^\\top X)^{-1}(X^\\top y)\n",
        "$$\n",
        "\n",
        "Here, we see that the solution depends on two quantities:\n",
        "\n",
        "* **The Linkage-Disequilibrium (LD) matrix** $X^\\top X$: This quantity is roughly similar across individuals of same ancestry, which means we can use publicly available data (e.g. 1000G Project) to estimate it.\n",
        "* **GWAS Summary Statistics** ($X^\\top y$): A measure that tells us about the correlation of the phenotype with each position in the genome. This can be approximated via Z-scores or p-values from GWAS studies.\n",
        "\n",
        "\n",
        "<center><img src=\"https://github.com/yueliyl/viprs-tut/blob/main/figures/sumstats.png?raw=1\" alt=\"drawing\" width=\"750\"/></center>\n",
        "\n",
        "In order for us to perform PRS inference from GWAS summary statistics, let's first go ahead and compute the LD matrix and perform GWAS on the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62be21c7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:13:59.597108Z",
          "start_time": "2024-04-12T16:13:59.270970Z"
        },
        "id": "62be21c7"
      },
      "outputs": [],
      "source": [
        "# Perform GWAS on the training data only:\n",
        "train.perform_gwas()\n",
        "\n",
        "# Compute LD matrix from training samples:\n",
        "train.compute_ld('sample',\n",
        "                 output_dir='./',\n",
        "                 dtype='float32')\n",
        "\n",
        "# -------- Visualize GWAS results: --------\n",
        "\n",
        "# Get GWAS summary statistics table:\n",
        "sumstats_table = train.to_summary_statistics_table()\n",
        "plt.scatter(sumstats_table.POS, -np.log10(sumstats_table.PVAL), marker='.')\n",
        "plt.xlabel(\"Genomic Position (Chromosome 22)\")\n",
        "plt.ylabel(\"-log10(P-Value)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "387e76dc",
      "metadata": {
        "id": "387e76dc"
      },
      "source": [
        "### (4.1) Naive PRS\n",
        "\n",
        "In the initial work on PRS leveraging GWAS summary statistics, the first approach simply used the marginal $\\beta$s inferred by the GWAS as the PRS weights. To be concrete, if the marginal betas from the GWAS are denoted as $\\beta^{(GWAS)}$s, then the PRS prediction under this model is simply:\n",
        "\n",
        "$$\n",
        "\\large \\hat{PRS}_i = \\sum_{j=1}^M X_{ij}\\beta^{(GWAS)}_j\n",
        "$$\n",
        "\n",
        "Since we just performed a GWAS, we can use the marginal BETAs to perform the prediction and see how well this approach fares in practice:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e203b1bc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:14:15.027425Z",
          "start_time": "2024-04-12T16:14:14.801944Z"
        },
        "id": "e203b1bc"
      },
      "outputs": [],
      "source": [
        "# Score using the marginal BETAs:\n",
        "\n",
        "naive_prs_pred = test.predict({22: train.sumstats_table[22].marginal_beta})\n",
        "\n",
        "# Let's compare the model prediction to the simulated phenotypes:\n",
        "plot_scatter(y_test, naive_prs_pred, title=\"NaivePRS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f692356",
      "metadata": {
        "id": "5f692356"
      },
      "source": [
        "As we can see, the `NaivePRS` approach is not as accurate as the models we derived from individual-level data. This is because this model assumes an infinitesimal architecture where all variants are causal and does not account for the fact that genetic variants tend to be highly correlated with each other. The latter fact implies that some non-causal variants may have large marginal effects by virtue of being correlated with a causal variant nearby in the genome.\n",
        "\n",
        "### (4.2) Clumping + Thresholding (C+T)\n",
        "\n",
        "An alternative approach to using all the marginal BETAs from GWAS is to restrict to **genome-wide significant variants**, e.g. those that pass the Bonferroni-correct P-Value threshold of $\\sim 5^{-8}$ (or some other user-provided threshold). This procedure is known as \"**Thresholding**\", because for variants that are above the p-value threshold, their effects will be set to zero.\n",
        "\n",
        "Thresholding on its own, however, will not tackle the issue of variants in a locus being highly correlated. So, the other procedure that we need to perform here is \"**Clumping**\", where, in each locus, we remove variants that are highly correlated (e.g. $r^2 > 0.9$) with the lead variant in that locus.\n",
        "\n",
        "In summary, for this method to work, the user has to specify **two parameters**:\n",
        "\n",
        "1) The P-value threshold for selecting \"significant\" variants.\n",
        "\n",
        "2) The $r^2$ threshold for clumping variants in LD with the lead variant.\n",
        "\n",
        "<center><img src=\"https://github.com/yueliyl/viprs-tut/blob/main/figures/PT.png?raw=1\" alt=\"drawing\" width=\"800\"/></center>\n",
        "\n",
        "The following code implements this using the GWAS data we derived earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d432974d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:14:20.694945Z",
          "start_time": "2024-04-12T16:14:20.270690Z"
        },
        "id": "d432974d"
      },
      "outputs": [],
      "source": [
        "# Import helper function to performing clumping:\n",
        "from magenpy.stats.ld.utils import clump_snps\n",
        "\n",
        "# Set the thresholds:\n",
        "pval_threshold = 1e-5\n",
        "rsq_threshold = 0.9\n",
        "\n",
        "# Step 1: Find which variants to keep by thresholding based on P-values:\n",
        "sumstats_table = train.to_summary_statistics_table()\n",
        "sumstats_table['Keep_T'] = sumstats_table.PVAL < pval_threshold\n",
        "\n",
        "# Step 2: Find which variants to keep after clumping variants that are highly correlated:\n",
        "clumped_snps = clump_snps(train.ld[22],\n",
        "                          statistic=sumstats_table.PVAL,\n",
        "                          rsq_threshold=rsq_threshold)\n",
        "# Create a dataframe with the clumped SNPs:\n",
        "clumped_snps = pd.DataFrame({'SNP': clumped_snps, 'Keep_C': True})\n",
        "# Merge with the original sumstats table:\n",
        "sumstats_table = sumstats_table.merge(clumped_snps, how='left')\n",
        "sumstats_table['Keep_C'] = sumstats_table['Keep_C'].infer_objects(copy=False).fillna(False)\n",
        "#.fillna(False).astype(bool)\n",
        "\n",
        "# Step 3: Combine both conditions:\n",
        "sumstats_table['Keep_Final'] = sumstats_table['Keep_T'] & sumstats_table['Keep_C']\n",
        "\n",
        "# Step 4: Compute the final BETA by taking into account both Clumping + Thresholding\n",
        "sumstats_table['CT_BETA'] = sumstats_table.BETA * sumstats_table.Keep_Final\n",
        "\n",
        "# Display the variants remaining after Clumping + Thresholding:\n",
        "\n",
        "print(\"Variants remaining after Clumping + Thresholding:\")\n",
        "display(sumstats_table.loc[sumstats_table.Keep_Final])\n",
        "\n",
        "# Score using the C+T betas:\n",
        "\n",
        "ct_prs_pred = test.predict({22: sumstats_table.CT_BETA})\n",
        "\n",
        "# Let's compare the model prediction to the simulated phenotypes:\n",
        "plot_scatter(y_test, ct_prs_pred, title=\"C+T PRS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cee3a51",
      "metadata": {
        "id": "9cee3a51"
      },
      "source": [
        "### (4.3) Bayesian PRS (`VIPRS`)\n",
        "\n",
        "The final major category of approaches that we will discuss in this context are those that use Bayesian inference techniques to infer polygenic risk score weights from GWAS summary statistics. Bayesian PRS methods operate by specifying priors distributions on the effect size, specifying the data likelihood, and then performing approximate Bayesian inference using Markov Chain Monte Carlo (MCMC) or Variational Inference (VI).\n",
        "\n",
        "In this presentation, we will discuss our recently proposed method for Bayesian PRS inference: **VIPRS**. `VIPRS` assigns a spike-and-slab prior on the effect size, as discussed in the previous section. Then, in order to approximate the Bayesian posterior, the method employs variational inference techniques to achieve fast and accurate posterior approximation. This is in contrast to many popular Bayesian PRS methods, which use Markov Chain Monte Carlo (MCMC) for posterior approximation.\n",
        "\n",
        "<center><img src=\"https://github.com/yueliyl/viprs-tut/blob/main/figures/mcmc.png?raw=1\" alt=\"drawing\" width=\"700\"/></center>\n",
        "\n",
        "<center><img src=\"https://github.com/yueliyl/viprs-tut/blob/main/figures/vi.png?raw=1\" alt=\"drawing\" width=\"700\"/></center>\n",
        "\n",
        "\n",
        "`VIPRS` was developed in conjunction with `magenpy`, so the interfaces should work together quite nicely. To perform inference on the summary statistics that we derived previously, you only need to pass the `train` object to `VIPRS` and then call `.fit` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab4810e9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:14:27.704231Z",
          "start_time": "2024-04-12T16:14:27.224186Z"
        },
        "id": "ab4810e9"
      },
      "outputs": [],
      "source": [
        "# Initialize the VIPRS object:\n",
        "v = vp.VIPRS(train)\n",
        "# Fit the model to data:\n",
        "v.fit()\n",
        "\n",
        "# Predict on the test samples:\n",
        "viprs_pred = v.predict(test)\n",
        "\n",
        "# Let's compare the model prediction to the simulated phenotypes:\n",
        "plot_scatter(y_test, viprs_pred, title=\"VIPRS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8704fcd",
      "metadata": {
        "id": "b8704fcd"
      },
      "source": [
        "In addition to the prediction accuracy on the held-out test set, we can also interrogate and inspect the behavior of the `VIPRS` model by examining the **Evidence Lower BOund (ELBO)**, the metric that the model is aiming to maximize:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56eb542b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:14:33.370994Z",
          "start_time": "2024-04-12T16:14:32.936880Z"
        },
        "id": "56eb542b"
      },
      "outputs": [],
      "source": [
        "from viprs.plot.diagnostics import plot_history\n",
        "\n",
        "plot_history(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da8e7e2f",
      "metadata": {
        "id": "da8e7e2f"
      },
      "source": [
        "One advantage of Bayesian methods like `VIPRS` is that we can also obtain interpretable outputs, such as\n",
        "the probability for given variant being causal for the phenotype of interest. This quantity is known in the\n",
        "literature as **Posterior Inclusion Probability (PIP)**. Since our phenotype here is simulated and we\n",
        "have a record of the true causal variants, we can compare the estimated **PIPs** to the causal status of the variants in the simulation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218807c8",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:14:38.282351Z",
          "start_time": "2024-04-12T16:14:38.044098Z"
        },
        "id": "218807c8"
      },
      "outputs": [],
      "source": [
        "# Export the inferred variant effect sizes:\n",
        "post_effect_table = v.to_table(col_subset=('CHR', 'SNP', 'POS', 'A1', 'A2'))\n",
        "\n",
        "display(post_effect_table)\n",
        "\n",
        "# Plot the Posterior Inclusion Probability (PIP):\n",
        "plt.scatter(post_effect_table.POS, post_effect_table.PIP,\n",
        "            marker='.', label='Non-causal variants')\n",
        "mask = train.get_causal_status()[22]\n",
        "plt.scatter(post_effect_table.POS[mask], post_effect_table.PIP[mask], color='red',\n",
        "            label='Causal variants')\n",
        "plt.xlabel(\"Genomic Position (Chromosome 22)\")\n",
        "plt.ylabel(\"Posterior Inclusion Probability (PIP)\")\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0a2cd09",
      "metadata": {
        "id": "b0a2cd09"
      },
      "source": [
        "### Other established PRS Methods that operate on GWAS Summary Statistics\n",
        "\n",
        "\n",
        "#### Bayesian:\n",
        "* [LDPred2](https://privefl.github.io/bigsnpr/articles/LDpred2.html)\n",
        "* [SBayesR](https://cnsgenomics.com/software/gctb/#Overview)\n",
        "* [PRScs](https://github.com/getian107/PRScs)\n",
        "\n",
        "#### Regularized Linear Models\n",
        "* [Lassosum](https://github.com/tshmak/lassosum)\n",
        "\n",
        "#### C+T\n",
        "* [plink2](https://www.cog-genomics.org/plink/2.0/)\n",
        "* [PRSice2](https://choishingwan.github.io/PRSice/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IH3nTlgKue4v",
      "metadata": {
        "id": "IH3nTlgKue4v"
      },
      "source": [
        "## üîπ (5) PRS Evaluation Strategies / Metrics\n",
        "\n",
        "Once a PRS model is fit to data, the next step is to evaluate its prediction accuracy on a held-out test set. Accuracy is measured differently depending on the type of phenotype considered (continuous vs. binary) as well as the data available about the validation / test cohort.\n",
        "\n",
        "\n",
        "For **continuous phenotypes**, common evaluation metrics include:\n",
        "\n",
        "* **Pearson Correlation Coefficient**: Measure the correlation between the PRS and the phenotype.\n",
        "* **R-Squared (Coefficient of Determination)**: The proportion of phenotypic variance expalined by the PRS.\n",
        "   * **Incremental R-Squared**: In research papers on PRS, the most common metric is incremental R-Squared, which takes the difference between the $R^2$ of a null model (with no PRS) and a full model with the PRS included. The null model is typically a linear model with only covariates (sex, age, Pricipal Components - PCs), whereas the full model is a linear model with these covariates plus the PRS:\n",
        "   \n",
        "   **Null model**: $y \\sim \\alpha_0 + \\sum_{c=1}^C C_k*\\beta_c$\n",
        "   \n",
        "   **Full model**: $y \\sim \\alpha_0 + PRS*\\beta_{prs} + \\sum_{c=1}^C C_k*\\beta_c$\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/yueliyl/viprs-tut/blob/main/figures/r2.png?raw=1\" alt=\"drawing\" width=\"500\"/>\n",
        "\n",
        "\n",
        "For **binary, case-control phenotypes**, common evaluation metrics include:\n",
        "\n",
        "* **Area Under the ROC Curve (AUROC)**: A measure of separability between cases and controls that ranges between 0.5 (random predictor) and 1 (perfect separability).\n",
        "* **Nagelkerke R-Squared**: A pseudo R-squared metric that works for binary traits (https://en.wikipedia.org/wiki/Pseudo-R-squared).\n",
        "* **Liability R-Squared**: A pseudo R-squared metric that is less biased than Nagelkerke (Lee et al., 2012).\n",
        "\n",
        "Reference:\n",
        "Lee, S. H., Goddard, M. E., Wray, N. R., & Visscher, P. M. (2012). A better coefficient of determination for genetic profile analysis. Genetic epidemiology, 36(3), 214-224.\n",
        "\n",
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/1920px-Roc_curve.svg.png\" alt=\"drawing\" width=\"400\"/></center>\n",
        "\n",
        "Source: [Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
        "\n",
        "These metrics provide an estimate of the predictive performance of the PRS methods. Many of these metrics are available with the `viprs` package and can be accessed as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9f527c5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-12T16:15:43.430864Z",
          "start_time": "2024-04-12T16:15:42.666387Z"
        },
        "id": "e9f527c5"
      },
      "outputs": [],
      "source": [
        "# import continuous metrics:\n",
        "\n",
        "from viprs.eval.continuous_metrics import incremental_r2, pearson_r\n",
        "\n",
        "print(\"Pearson Correlation (VIPRS):\", pearson_r(y_test, viprs_pred))\n",
        "\n",
        "# NOTE: In this case, because we don't have covariates, the null model\n",
        "# is a model with only intercept:\n",
        "print(\"Incremental R-Squared (VIPRS):\", incremental_r2(y_test, viprs_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e0203be",
      "metadata": {
        "id": "5e0203be"
      },
      "source": [
        "A full listing of PRS evaluation metrics and their implementation in `python` is available [here](https://shz9.github.io/viprs/api/overview/#model-evaluation)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a23887c2",
      "metadata": {
        "id": "a23887c2"
      },
      "source": [
        "## üîπ (6) Inference of Polygenic Risk Scores (PRS) from Publicly Available GWAS Summary Statistics\n",
        "\n",
        "Refer to https://github.com/shz9/viprs/blob/master/notebooks/viprs_cli_example.ipynb\n",
        "\n",
        "For commandline tools, refer to tutorials / references on the `viprs` [documentation](https://shz9.github.io/viprs/) page."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cc0394f",
      "metadata": {
        "id": "1cc0394f"
      },
      "source": [
        "## üóÇÔ∏è References / Further reading\n",
        "\n",
        "* [Common polygenic variation contributes to risk of schizophrenia and bipolar disorder](https://doi.org/10.1038/nature08185)\n",
        "* [PRSice-2: Polygenic Risk Score software for biobank-scale data](https://doi.org/10.1093/gigascience/giz082)\n",
        "* [A guide to performing Polygenic Risk Score analyses](https://doi.org/10.1101/416545)\n",
        "* [Polygenic risk scores: from research tools to clinical instruments](https://doi.org/10.1186/s13073-020-00742-5)\n",
        "* [Modeling Linkage Disequilibrium Increases Accuracy of Polygenic Risk Scores](https://doi.org/10.1016%2Fj.ajhg.2015.09.001)\n",
        "* [LDpred2: better, faster, stronger](https://doi.org/10.1093/bioinformatics/btaa1029)\n",
        "* [Polygenic prediction via Bayesian regression and continuous shrinkage priors](https://doi.org/10.1038/s41467-019-09718-5)\n",
        "* [Improved polygenic prediction by Bayesian multiple regression on summary statistics](https://doi.org/10.1038/s41467-019-12653-0)\n",
        "* [Fast and accurate Bayesian polygenic risk modeling with variational inference](https://doi.org/10.1016/j.ajhg.2023.03.009)\n",
        "* [Toward whole-genome inference of polygenic scores with fast and memory-efficient algorithms](https://doi.org/10.1016/j.ajhg.2025.05.002)\n",
        "* [Sparse Polygenic Risk Score Inference with the Spike-and-Slab LASSO](https://www.medrxiv.org/content/10.1101/2025.01.28.25321292v1)\n",
        "* [A fast and scalable framework for large-scale and ultrahigh-dimensional sparse regression with application to the UK Biobank](https://doi.org/10.1371/journal.pgen.1009141)\n",
        "* [Scalable Variational Inference for Bayesian Variable Selection in Regression, and Its Accuracy in Genetic Association Studies](https://doi.org/10.1214/12-BA703)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4bfb1aa",
      "metadata": {
        "id": "d4bfb1aa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}